## Elegant sentences to explain weakness
* Convolutional architectures have proven **extremely** successful for vision tasks. 
Their hard inductive enable sample-efficient learning, but **comes at the cost of** 
a **potentially** lower-performance ceiling. Vision Transformers (ViTs) **rely on** more
flexible self-attention layers, and have recently outperformed CNNs for image
Classification. However, they require costly pre-training on large external 
datasets or distillation from pretrained convolutional networks.