

## Questions before diving into transformers

* What is transformer and what are the basic components?
* What is the difference between it with CNN and other methods?
* What is the challenge to deploy transformer in different applications?
* What kinds of applications can we use transformer?
* What are the current attentions regrading transformer? 
* What are the changes about transformerand the basic assuptions behind them?

## Questions in 2021.09.05 after talking with Shuli
* Is it required to have Q, K V in transformer? Why should we have the three elements?
  * The basic difference between transformer and multilayer perceptron?
* How to push computer vision to achieve unsupervised learning? Can transformer give us an alternative paradigm for it and how?
* Is there any other paradigm to use transformer than combining CNN and transformer?
* If we want to combine CNN and transformer, what should we persist and change? Can I make a taxonomy for them?
* Basic insights from CNN and transformer
  * Hierarchical feature and multi-scale
  * Tranlate invariance
  * Feature evolution and depth of basic blocks
  * Global dependency
  * Optimization issue and residual connection
* How to make images into sequence?
  * Can we use sequence but keep the spatial info for images?
  * The difference of sequence between NLP and CV?
* 
